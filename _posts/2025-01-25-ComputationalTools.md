---
title:  "Computational tools in psycholinguistic studies"
mathjax: true
layout: post
categories: media
---

With the rise of large language models (LLMs), computational methods are becoming increasingly popular in psycholinguistics. This post gathers key resources on using LLM-based surprisal for syntactic ambiguity research, as well as tutorials on training and evaluating these models.

Word surprisal is a commonly used indicator of processing difficulty. Check out [a unified interface for computing surprisal from language models](https://github.com/aalok-sathe/surprisal) and [tools for calculating psycholinguistically-relevant metrics of language statistics using transformer language models](https://github.com/jmichaelov/PsychFormers).

> Check out [this paper](https://www.sciencedirect.com/science/article/abs/pii/S0749596X24000135) for a large-scale dataset on using LLMs surprisal to explain syntactic disambiguation difficulty: [Syntactic Ambiguity Processing Benchmark](https://github.com/caplabnyu/sapbenchmark)

This [tutorials and resources on LLMs training and evaluation](https://sarehalli.github.io/resources) from Professor Suhas Arehalli can be helpful in model training and manipulation.
